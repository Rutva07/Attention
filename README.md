# About

This project creates language models via building a Masked Language Model, where a language model is trained to predict a “masked” word that is missing from a sequence of text. This program has two features:

First, we’ll use the transformers Python library, developed by AI software company Hugging Face, to write a program that uses BERT to predict masked words. The program will also generate diagrams visualizing attention scores.

Second, we’ll analyze the diagrams generated by our program to try to understand what BERT’s attention heads might be paying attention to as it attempts to understand our natural language.
